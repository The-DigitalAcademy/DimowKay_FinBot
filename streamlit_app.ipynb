{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##âœ… Streamlit Code for FinBot:\n",
        "- Load fine-tuned model using transformers + bitsandbytes\n",
        "\n",
        "- Build a Streamlit interface\n",
        "\n",
        "- Set up a function to generate answers\n",
        "\n",
        "- Run the chatbot in a browser"
      ],
      "metadata": {
        "id": "FFSUdfBd7ELY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# App title and instructions\n",
        "st.set_page_config(page_title=\"FinBot - Financial Literacy Chatbot\")\n",
        "st.title(\"ðŸ’° FinBot - Your Financial Literacy Assistant\")\n",
        "st.markdown(\"Ask me anything about budgeting, saving, investing, or credit!\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model_id = \"0xroyce/Plutus-3B\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        load_in_8bit=True,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = load_model()\n",
        "\n",
        "# Function to generate response\n",
        "def get_response(question):\n",
        "    input_ids = tokenizer.encode(question, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.2,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(question)[-1].strip()\n",
        "\n",
        "# Chat input and history\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = []\n",
        "\n",
        "user_input = st.text_input(\"ðŸ’¬ Enter your financial question here:\")\n",
        "\n",
        "if user_input:\n",
        "    with st.spinner(\"Thinking...\"):\n",
        "        answer = get_response(user_input)\n",
        "        st.session_state.chat_history.append((user_input, answer))\n",
        "\n",
        "# Show chat history\n",
        "for user_msg, bot_msg in reversed(st.session_state.chat_history):\n",
        "    st.markdown(f\"**You:** {user_msg}\")\n",
        "    st.markdown(f\"**FinBot:** {bot_msg}\")\n",
        "    st.markdown(\"---\")\n"
      ],
      "metadata": {
        "id": "JyT5bPhG7WGl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}