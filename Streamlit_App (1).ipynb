{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Streamlit UI\n",
        "Create a new file finbot_app.py:\n",
        "\n",
        "##Explanation:\n",
        "- @st.cache_resource caches the model to avoid reloading\n",
        "\n",
        "- We recreate the conversation chain with the fine-tuned model\n",
        "\n",
        "- Streamlit provides a chat interface with message history\n",
        "\n",
        "- User inputs are added to conversation history and displayed\n",
        "\n",
        "- The model generates responses that are displayed in the UI"
      ],
      "metadata": {
        "id": "_1zfl1f2gN6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"./finbot_finetuned\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"./finbot_finetuned\",\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return pipe\n",
        "\n",
        "pipe = load_model()\n",
        "\n",
        "# Set up LangChain\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(llm=pipe, memory=memory)\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"FinBot - Financial Literacy Assistant\")\n",
        "st.write(\"Ask me anything about personal finance!\")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Accept user input\n",
        "if prompt := st.chat_input(\"What financial question do you have?\"):\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    # Display user message\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Get bot response\n",
        "    response = conversation.predict(input=prompt)\n",
        "\n",
        "    # Display bot response\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
      ],
      "metadata": {
        "id": "W25uNefugk6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running the Application\n",
        "running the Streamlit app from Colab:\n",
        "\n",
        "##Explanation:\n",
        "- Since Colab doesn't directly expose web apps, we use localtunnel\n",
        "\n",
        "- This will give you a public URL to access your Streamlit app\n",
        "\n",
        "- The app runs on port 8501 (default Streamlit port)"
      ],
      "metadata": {
        "id": "7CVQordvg4De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run finbot_app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "e0_UlkhmhIrt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}