# -*- coding: utf-8 -*-
"""FinBot_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BFvXK6aziFJ2pYn6PeE1UGQ1-eB4Wtbz

## Project Overview:

Problem:

- Financial education resources are often complex, scattered, and lack
  personalization.

- People in underserved communities donâ€™t always have access to credible or
  easily understandable advice.

- Traditional learning methods are not interactive or real-time, limiting
  engagement.

Solution:

Build FinBot, a conversational AI chatbot that:

- Uses transformer-based NLP models (like DistilBERT) to answer personal
  finance questions.

- Leverages a custom Q&A dataset for fine-tuning.

- Is accessible, real-time, and covers topics such as:

-    Budgeting

-    Saving

-    Credit management

-    Loans

-    Investing

-    Insurance
"""

!pip install transformers datasets torch streamlit

"""# Preprocess the dataset for tokenization and prepare it for training.

We will preprocess the dataset to tokenize the questions and answers for both BERT (classification) and GPT (generative tasks).
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import AutoTokenizer


train_data = pd.read_csv('/content/train_data.csv')
val_data = pd.read_csv('/content/val_data.csv')
test_data = pd.read_csv('/content/test_data.csv')


train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_data['question'], train_data['answer'], test_size=0.2, random_state=42
)


all_labels = pd.concat([train_labels, val_labels])


label_encoder = LabelEncoder()
label_encoder.fit(all_labels)


train_labels_encoded = label_encoder.transform(train_labels)
val_labels_encoded = label_encoder.transform(val_labels)


bert_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
train_encodings_bert = bert_tokenizer(list(train_texts), truncation=True, padding=True, max_length=512)
val_encodings_bert = bert_tokenizer(list(val_texts), truncation=True, padding=True, max_length=512)


gpt_tokenizer = AutoTokenizer.from_pretrained("gpt2")
gpt_tokenizer.pad_token = gpt_tokenizer.eos_token
train_encodings_gpt = gpt_tokenizer(list(train_texts), truncation=True, padding=True, max_length=512)
val_encodings_gpt = gpt_tokenizer(list(val_texts), truncation=True, padding=True, max_length=512)

""" # Prepare Data for PyTorch:

Convert tokenized data into PyTorch datasets.
"""

import torch

# Dataset for BERT (classification tasks)
class FinancialDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

# Create datasets for BERT
train_dataset_bert = FinancialDataset(train_encodings_bert, train_labels_encoded)
val_dataset_bert = FinancialDataset(val_encodings_bert, val_labels_encoded)

# Dataset for GPT (generative tasks)
class FinancialDatasetGPT(torch.utils.data.Dataset):
    def __init__(self, tokenizer, texts, labels, max_length=512):
        self.tokenizer = tokenizer
        self.texts = texts
        self.labels = labels
        self.max_length = max_length

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        # Tokenize input text
        inputs = self.tokenizer(
            self.texts[idx],
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        # Tokenize labels
        labels = self.tokenizer(
            self.labels[idx],
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        # Convert to tensors and return
        item = {key: val.squeeze(0) for key, val in inputs.items()}
        item['labels'] = labels['input_ids'].squeeze(0)
        return item

# Create datasets for GPT
train_dataset_gpt = FinancialDatasetGPT(
    tokenizer=gpt_tokenizer,
    texts=list(train_texts),
    labels=list(train_labels),
    max_length=512,
)

val_dataset_gpt = FinancialDatasetGPT(
    tokenizer=gpt_tokenizer,
    texts=list(val_texts),
    labels=list(val_labels),
    max_length=512,
)

"""
 # Fine-Tune BERT:
We will fine-tune a pretrained BERT model for classification tasks."""

from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments

# Load BERT model for classification
bert_model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=len(label_encoder.classes_))

# Define training arguments
training_args_bert = TrainingArguments(
    output_dir='./results_bert',
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    report_to="none",
)

# Fine-tune BERT
trainer_bert = Trainer(
    model=bert_model,
    args=training_args_bert,
    train_dataset=train_dataset_bert,
    eval_dataset=val_dataset_bert,
)

trainer_bert.train()

"""# Fine-Tune GPT:

We will fine-tune a pretrained GPT model for generative tasks.
"""

from transformers import AutoModelForCausalLM, Trainer, TrainingArguments

# Load GPT model for generative tasks
gpt_model = AutoModelForCausalLM.from_pretrained("gpt2")

# Define training arguments
training_args_gpt = TrainingArguments(
    output_dir='./results_gpt',
    eval_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    report_to="none",
)

# Fine-tune GPT
trainer_gpt = Trainer(
    model=gpt_model,
    args=training_args_gpt,
    train_dataset=train_dataset_gpt,
    eval_dataset=val_dataset_gpt,
)

trainer_gpt.train()

import pickle

# Save the trained label encoder
with open("label_encoder.pkl", "wb") as f:
    pickle.dump(label_encoder, f)

""" # Evaluate the Models:

Evaluate BERT using accuracy/F1-score and GPT using BLEU score.


"""

from sklearn.metrics import accuracy_score, f1_score
import numpy as np

# Get predictions from BERT
predictions = trainer_bert.predict(val_dataset_bert)
preds = np.argmax(predictions.predictions, axis=1)
labels = predictions.label_ids

# Accuracy & F1
accuracy = accuracy_score(labels, preds)
f1 = f1_score(labels, preds, average='weighted')  # Use 'macro' if all classes are equally important

print("ðŸ“Š BERT Model Evaluation:")
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

# Evaluate a few samples
smoothie = SmoothingFunction().method4
scores = []

for i in range(100):  # Evaluate first 100 samples for speed
    input_ids = gpt_tokenizer.encode(train_texts.iloc[i], return_tensors="pt")
    output_ids = gpt_model.generate(input_ids, max_length=100)
    generated = gpt_tokenizer.decode(output_ids[0], skip_special_tokens=True)

    reference = [train_labels.iloc[i].split()]
    candidate = generated.split()

    score = sentence_bleu(reference, candidate, smoothing_function=smoothie)
    scores.append(score)

avg_bleu = np.mean(scores)
print("ðŸ“š GPT Model Evaluation:")
print(f"Average BLEU Score: {avg_bleu:.4f}")

""" # Save the Fine-Tuned BERT Model"""

# Save the fine-tuned BERT model
bert_model.save_pretrained("./bert_finbot")
bert_tokenizer.save_pretrained("./bert_finbot")

"""# Save the Fine-Tuned GPT Model"""

# Save the fine-tuned GPT model
gpt_model.save_pretrained("./gpt_finbot")
gpt_tokenizer.save_pretrained("./gpt_finbot")

"""# Load the Saved BERT Model
To load the saved BERT model and tokenizer for deployment:
"""

from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Load the fine-tuned BERT model
bert_model = AutoModelForSequenceClassification.from_pretrained("./bert_finbot")
bert_tokenizer = AutoTokenizer.from_pretrained("./bert_finbot")

"""# Load the Saved GPT Model
To load the saved GPT model and tokenizer for deployment:
"""

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the fine-tuned GPT model
gpt_model = AutoModelForCausalLM.from_pretrained("./gpt_finbot")
gpt_tokenizer = AutoTokenizer.from_pretrained("./gpt_finbot")

"""# Deploy the Models Using Streamlit:

Create a Streamlit app to interact with the models.
"""

# # Write the Streamlit app to a file
# %%writefile app.py
# import streamlit as st
# from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM

# # Load models and tokenizers
# bert_model = AutoModelForSequenceClassification.from_pretrained("./bert_finbot")
# bert_tokenizer = AutoTokenizer.from_pretrained("./bert_finbot")

# gpt_model = AutoModelForCausalLM.from_pretrained("./gpt_finbot")
# gpt_tokenizer = AutoTokenizer.from_pretrained("./gpt_finbot")

# st.title("FinBot - Conversational Financial Literacy Assistant")

# # User input
# user_input = st.text_input("Ask a financial question:")

# if st.button("Get Answer"):
#     # Use BERT for classification
#     bert_inputs = bert_tokenizer(user_input, return_tensors="pt", truncation=True, padding=True)
#     bert_outputs = bert_model(**bert_inputs)
#     st.write("BERT Model Output:", bert_outputs.logits)

#     # Use GPT for generative response
#     gpt_inputs = gpt_tokenizer.encode(user_input, return_tensors="pt")
#     gpt_outputs = gpt_model.generate(gpt_inputs, max_length=100, num_return_sequences=1)
#     gpt_response = gpt_tokenizer.decode(gpt_outputs[0], skip_special_tokens=True)
#     st.write("GPT Model Response:", gpt_response)

# # Install required libraries
# !pip install streamlit pyngrok

# # Run Streamlit in the background
# !streamlit run app.py &

# # Expose the app using Ngrok
# from pyngrok import ngrok
# public_url = ngrok.connect(port=8501)
# print(f"Streamlit app is live at: {public_url}")

# import streamlit as st
# import torch
# import numpy as np
# from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM
# from sklearn.preprocessing import LabelEncoder
# import pickle

# # Load models
# bert_model = AutoModelForSequenceClassification.from_pretrained("./bert_finbot")
# bert_tokenizer = AutoTokenizer.from_pretrained("./bert_finbot")
# gpt_model = AutoModelForCausalLM.from_pretrained("./gpt_finbot")
# gpt_tokenizer = AutoTokenizer.from_pretrained("./gpt_finbot")

# # Load label encoder
# with open("label_encoder.pkl", "rb") as f:
#     label_encoder = pickle.load(f)

# st.title("ðŸ’° FinBot - Conversational Financial Literacy Assistant")
# st.markdown("Ask me anything about **saving, budgeting, credit, or investing** ðŸ“ˆ")

# user_input = st.text_input("Type your financial question:")

# if st.button("Get Answer") and user_input.strip() != "":
#     with st.spinner("Thinking..."):

#         # ---------- BERT Classification ----------
#         inputs_bert = bert_tokenizer(user_input, return_tensors="pt", truncation=True, padding=True)
#         outputs_bert = bert_model(**inputs_bert)
#         probs = torch.nn.functional.softmax(outputs_bert.logits, dim=1)
#         predicted_class = torch.argmax(probs, dim=1).item()
#         answer = label_encoder.inverse_transform([predicted_class])[0]

#         # ---------- GPT Generation ----------
#         inputs_gpt = gpt_tokenizer.encode(user_input, return_tensors="pt")
#         outputs_gpt = gpt_model.generate(inputs_gpt, max_length=100, num_return_sequences=1, pad_token_id=gpt_tokenizer.eos_token_id)
#         gpt_response = gpt_tokenizer.decode(outputs_gpt[0], skip_special_tokens=True)

#         # Display results
#         st.subheader("ðŸ¤– BERT Answer (Based on Classification):")
#         st.success(answer)

#         st.subheader("ðŸ§  GPT Answer (Generative):")
#         st.info(gpt_response)

with open("label_encoder.pkl", "wb") as f:
    pickle.dump(label_encoder, f)

!pip install streamlit pyngrok

import streamlit as st
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM
from sklearn.preprocessing import LabelEncoder
import pickle

# Load models
bert_model = AutoModelForSequenceClassification.from_pretrained("./bert_finbot")
bert_tokenizer = AutoTokenizer.from_pretrained("./bert_finbot")
gpt_model = AutoModelForCausalLM.from_pretrained("./gpt_finbot")
gpt_tokenizer = AutoTokenizer.from_pretrained("./gpt_finbot")

# Load label encoder
with open("label_encoder.pkl", "rb") as f:
    label_encoder = pickle.load(f)

st.title("ðŸ’° FinBot - Conversational Financial Literacy Assistant")
st.markdown("Ask me anything about **saving, budgeting, credit, or investing** ðŸ“ˆ")

user_input = st.text_input("Type your financial question:")

if st.button("Get Answer") and user_input.strip() != "":
    with st.spinner("Thinking..."):

        # ---------- BERT Classification ----------
        inputs_bert = bert_tokenizer(user_input, return_tensors="pt", truncation=True, padding=True)
        outputs_bert = bert_model(**inputs_bert)
        probs = torch.nn.functional.softmax(outputs_bert.logits, dim=1)
        predicted_class = torch.argmax(probs, dim=1).item()
        answer = label_encoder.inverse_transform([predicted_class])[0]

        # ---------- GPT Generation ----------
        inputs_gpt = gpt_tokenizer.encode(user_input, return_tensors="pt")
        outputs_gpt = gpt_model.generate(inputs_gpt, max_length=100, num_return_sequences=1, pad_token_id=gpt_tokenizer.eos_token_id)
        gpt_response = gpt_tokenizer.decode(outputs_gpt[0], skip_special_tokens=True)

        # Display results
        st.subheader("ðŸ¤– BERT Answer (Based on Classification):")
        st.success(answer)

        st.subheader("ðŸ§  GPT Answer (Generative):")
        st.info(gpt_response)

!streamlit run app.py &>/content/logs.txt &

from pyngrok import ngrok

!ngrok config add-authtoken 2vXAyMuKlu6eUIWs7HzK3sBrHUE_7m82W5jEJwqbjSyhwXtT

from pyngrok import ngrok

# Correct way to connect in ngrok v3+
public_url = ngrok.connect("http://localhost:8501")
print(f"Streamlit app is live at: {public_url}")

"""# Steps to build the model:


FinBot â€“ Conversational Financial Literacy Assistant: aims to create an AI-powered chatbot to provide personalized, real-time financial guidance.


The solution involves training a model on a curated dataset of financial questions and answers, leveraging NLP and machine learning.



Hereâ€™s how iâ€™ll proceed to address the problem:



Steps to Build FinBot



Data Preprocessing:

- Tokenize the dataset (questions and answers).

- Convert labels (answers) into a format suitable for training.

- Split the dataset into training and validation sets.





Model Selection:

- Use BERT for understanding the context of financial questions.

- Use GPT for generating conversational, human-like responses.





Fine-Tuning:

- Fine-tune pretrained BERT and GPT models using Hugging Face Transformers.

- Ensure the models are trained on your curated dataset.





Evaluation:

- Evaluate the models using metrics like BLEU (for GPT) and accuracy/F1-score (for BERT).





Deployment:

Deploy the chatbot using Streamlit for an interactive user interface.






Key Adjustments for Your Problem

- Dataset: Your dataset contains financial questions and answers. We'll preprocess it to ensure compatibility with both BERT (for classification tasks) and GPT (for generative tasks).


- Interactivity: The chatbot will allow users to ask financial questions and receive personalized, easy-to-understand answers.

- Real-Time Responses: GPT will handle conversational responses, while BERT can classify or retrieve relevant answers.
"""

